---
title: "R Notebook"
output: html_notebook
---

KNN is k-nearest neighbor classification algorithm. KNN is a supervised algorithm.This deal with classification problems.

#Limitations--  
1) If data is noisy hard to find the clear boundary.   
2) Need nice k  
3)Lasy learner-Do you feel it is learning from training data. No  
4) Missing data need extra effort  

Example for KNN algorithm-- suppose we have 4 cities in the two dim space. A(2,3), B(3,4), C(1,3) and D(4,6). This are in our training model.let a test city is T(2,2). Given A and B in SF and C and D in San Jose.

Distance of A from T=sqrt((2-2)^2 + (3-2)^2)=1  
Distance of B from T=sqrt((3-2)^2 + (4-2)^2)=2.2360  
Distance of C from T=sqrt((1-2)^2 + (3-2)^2)=1.414  
Distance of D from T=sqrt((4-2)^2 + (6-2)^2)=4.4721  

If we will take k=1, A will be nearest to T.T will be in SF
If we will take k=2, It will take A and C, so even number is a bad option in KNN.
If we will take k=3, It will take A, C, B majority is in SF so T will be in SF.

A larger selection of k is a bad option for noisy data. Mostly k is used as sqrt(n), where n is the number of observations in training dataset.

As we are dealing with the distance. we should use the same scale for all variables otherwise it will mislead our algorithms.

KNN use Euclidean distance for distance calculation.

```{r}
#Distance calculation
sqrt((4-2)^2+(6-2)^2)
```
```{r}
#Reading data
data1=iris
str(data1)
```

Iris data has 4 numerical and 1 categorical variables.
Something interesting about iris data.
[link](https://en.wikipedia.org/wiki/Iris_(plant))

```{r}
#Number of species
table(data1$Species)
```

```{r}
#Summary of numerical variables 
summary(data1)
```
As we can see that scale is not same for all three variables, so our first task is to make scale same.
```{r}
#Lets normalize the data
n=function(y) {return((y-min(y))/(max(y)-min(y))) }
data1_n=as.data.frame(lapply(data1[-5],n))

#Summary of normalized data
summary(data1_n)

#Lets try Z score
data1_z=as.data.frame(scale(data1[-5]))
```
value are in the same scale.
```{r}
#Lets divide data into training and test
set.seed(12)
sam=sample(150,110,replace = FALSE)
sam
train1=data1_n[sam,]
test1=data1_n[-sam,]

train2=data1_z[sam,]
test2=data1_z[-sam,]

#Creating labels 
train_label=data1[sam,5]
test_label=data1[-sam,5]
```

```{r}
#KNN model with k=1
library(class)
pred1=knn(train = train1,test=test1, cl=train_label , k=1)

#Model performance 
library(gmodels)
CrossTable(x=test_label,y=pred1,prop.chisq = FALSE)
```
```{r}
#Function for accuracy
accuracy=function(actual,prediction){mean(actual==prediction)}
a1=accuracy(test_label,pred1)
a1
```

```{r}
#KNN model with z score transformation
pred2=knn(train = train2,test=test2 , cl=train_label ,k=1)
CrossTable(x=pred2,y=test_label,prop.chisq = FALSE)
```

```{r}
#Checking for accuracy
a2=accuracy(test_label,pred2)
a2
```


```{r}
#KNN model with K=3
pred3=knn(train = train1,test=test1, cl=train_label , k=3)
CrossTable(x=test_label,y=pred3,prop.chisq = FALSE)
```
```{r}
#Checking for accuracy
a3=accuracy(test_label,pred3)
a3
```


```{r}
#KNN model with k=5
pred4=knn(train = train1,test=test1, cl=train_label , k=5)
CrossTable(x=test_label,y=pred4,prop.chisq = FALSE)
```

```{r}
#Checking for accuracy
a4=accuracy(test_label,pred4)
a4
```

```{r}
#KNN model with k=7
pred5=knn(train = train1,test=test1 , cl=train_label ,k=7)
CrossTable(x=test_label , y=pred5, prop.chisq = FALSE)

```
```{r}
#Checking for accuracy
a5=accuracy(test_label,pred5)
a5
```


```{r}
#KNN model with k=9
pred6=knn(train = train1,test=test1 , cl=train_label ,k=9)
CrossTable(x=test_label , y=pred6, prop.chisq = FALSE)
```
```{r}
#Checking for accuracy
a6=accuracy(test_label,pred6)
a6
```



```{r}
accuracy=c(a1,a3,a4,a5,a6)
k_values=c(1,3,5,7,9)
library(ggplot2)
data2=cbind.data.frame(accuracy,k_values)
ggplot(data2,aes(y=accuracy,x=k_values))+geom_smooth(col="red")
```
KKN is non-parametric algorithm. It does not keep any assumption for data distribution.and it does not effected by outliers.

```{r}
library(alr4)
data3=alr4::UN11
str(data3)
```

We have 2 factor variable and 4 numerical variables. our preditor variable is life expectancy.
```{r}
#Converting factors to numeric variables 
data3$region=as.integer(as.factor(data3$region))
data3$group=as.integer(as.factor(data3$group))
str(data3)

```

```{r}
data3_n=as.data.frame(lapply(data3,n))

#Training and tes data
sample1=sample(199,120,replace = FALSE)
data3_train=data3_n[sample1,-5]
data3_test=data3_n[-sample1,-5]
dim(data3_train)
dim(data3_test)

#Create labels
data3_train_labels=as.data.frame(data3_n[sample1,5])
data3_test_labels=as.data.frame(data3_n[-sample1,5])

#Apply KNN-Regression
library(class)
library(FNN)
pred21=knn.reg(train=data3_train,test =data3_test,y=data3_train_labels,k=1,algorithm=c("kd_tree"))
typeof(pred21)
#Converting list of pred to dataframe 
pred21_d= cbind.data.frame(matrix(unlist(pred21)))

#Deleting usused values 
pred_22=data.frame(pred21_d)
pred_44=data.frame(pred_22[-c(1:3),])
pred_55=data.frame(t(pred_44))

dim(pred_55)
dim(data3_test_labels)

```
```{r}
#Creating Dataframe with test and prediction
data_pred=cbind.data.frame(pred_55,data3_test_labels)

```

```{r}
#Actual and predicted
ggplot(data=data_pred,aes(x=t.pred_44.,y=`data3_n[-sample1, 5]`))+geom_smooth(col="red")+xlab("predicted")+ylab("Actual")
```









